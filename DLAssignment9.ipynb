{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Assignment 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rachit Aggarwal - M12506500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rachi\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing placeholders for both DNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28 # MNIST\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, 2, n_inputs), name=\"X\")\n",
    "X1, X2 = tf.unstack(X, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing placeholder for label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.int32, shape=[None, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining DNN function with He initialization and ELU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "def dnn(inputs, n_hidden_layers=5, n_neurons=100, name=None,\n",
    "        activation=tf.nn.elu, initializer=he_init):\n",
    "    with tf.variable_scope(name, \"dnn\"):\n",
    "        for layer in range(n_hidden_layers):\n",
    "            inputs = tf.layers.dense(inputs, n_neurons, activation=activation,\n",
    "                                     kernel_initializer=initializer,\n",
    "                                     name=\"hidden%d\" % (layer + 1))\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding inputs to two separate DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnn1 = dnn(X1, name=\"DNN_A\")\n",
    "dnn2 = dnn(X2, name=\"DNN_B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnn_outputs = tf.concat([dnn1, dnn2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Each DNN outputs 100 activations (per instance), so the shape is [None, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(100)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(100)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concatenated outputs have a shape of [None, 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(200)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inserting new hidden layer with 10 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden = tf.layers.dense(dnn_outputs, units=10, activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "logits = tf.layers.dense(hidden, units=1, kernel_initializer=he_init)\n",
    "y_proba = tf.nn.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting values based on logits values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = tf.cast(tf.greater_equal(logits, 0), tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_as_float = tf.cast(y, tf.float32)\n",
    "xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_as_float, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "momentum = 0.95\n",
    "\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## measuring the accuracy of classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_correct = tf.equal(y_pred, y)\n",
    "accuracy = tf.reduce_mean(tf.cast(y_pred_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 10.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating training, validation and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train1 = mnist.train.images\n",
    "y_train1 = mnist.train.labels\n",
    "\n",
    "X_train2 = mnist.validation.images\n",
    "y_train2 = mnist.validation.labels\n",
    "\n",
    "X_test = mnist.test.images\n",
    "y_test = mnist.test.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to generate pair of images 50% for each digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(images, labels, batch_size):\n",
    "    size1 = batch_size // 2\n",
    "    size2 = batch_size - size1\n",
    "    if size1 != size2 and np.random.rand() > 0.5:\n",
    "        size1, size2 = size2, size1\n",
    "    X = []\n",
    "    y = []\n",
    "    while len(X) < size1:\n",
    "        rnd_idx1, rnd_idx2 = np.random.randint(0, len(images), 2)\n",
    "        if rnd_idx1 != rnd_idx2 and labels[rnd_idx1] == labels[rnd_idx2]:\n",
    "            X.append(np.array([images[rnd_idx1], images[rnd_idx2]]))\n",
    "            y.append([1])\n",
    "    while len(X) < batch_size:\n",
    "        rnd_idx1, rnd_idx2 = np.random.randint(0, len(images), 2)\n",
    "        if labels[rnd_idx1] != labels[rnd_idx2]:\n",
    "            X.append(np.array([images[rnd_idx1], images[rnd_idx2]]))\n",
    "            y.append([0])\n",
    "    rnd_indices = np.random.permutation(batch_size)\n",
    "    return np.array(X)[rnd_indices], np.array(y)[rnd_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch of 5 image pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "X_batch, y_batch = generate_batch(X_train1, y_train1, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 2, 784), dtype('float32'))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch.shape, X_batch.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the pair of imgaes from the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANMAAAGfCAYAAADF6ud6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwmIiMsJiIjLyf/462If8gG2RAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1dd596453c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(3, 3 * batch_size))\n",
    "plt.subplot(121)\n",
    "plt.imshow(X_batch[:,0].reshape(28 * batch_size, 28), cmap=\"binary\", interpolation=\"nearest\")\n",
    "plt.axis('off')\n",
    "plt.subplot(122)\n",
    "plt.imshow(X_batch[:,1].reshape(28 * batch_size, 28), cmap=\"binary\", interpolation=\"nearest\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checking the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 10.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a test set of many pairs of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test1, y_test1 = generate_batch(X_test, y_test, batch_size=len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train loss: 0.60465544\n",
      "0 Test accuracy: 0.6997\n",
      "1 Train loss: 0.36080956\n",
      "2 Train loss: 0.30109534\n",
      "3 Train loss: 0.2616911\n",
      "4 Train loss: 0.21387331\n",
      "5 Train loss: 0.1875908\n",
      "5 Test accuracy: 0.9159\n",
      "6 Train loss: 0.21559455\n",
      "7 Train loss: 0.18048915\n",
      "8 Train loss: 0.18577994\n",
      "9 Train loss: 0.16092974\n",
      "10 Train loss: 0.12513581\n",
      "10 Test accuracy: 0.9375\n",
      "11 Train loss: 0.17866379\n",
      "12 Train loss: 0.1242573\n",
      "13 Train loss: 0.11760525\n",
      "14 Train loss: 0.09440907\n",
      "15 Train loss: 0.09654444\n",
      "15 Test accuracy: 0.9521\n",
      "16 Train loss: 0.14447895\n",
      "17 Train loss: 0.09864418\n",
      "18 Train loss: 0.11442176\n",
      "19 Train loss: 0.12862927\n",
      "20 Train loss: 0.124105126\n",
      "20 Test accuracy: 0.96\n",
      "21 Train loss: 0.073349416\n",
      "22 Train loss: 0.102564454\n",
      "23 Train loss: 0.09699113\n",
      "24 Train loss: 0.117153294\n",
      "25 Train loss: 0.0812609\n",
      "25 Test accuracy: 0.9651\n",
      "26 Train loss: 0.08908055\n",
      "27 Train loss: 0.063645974\n",
      "28 Train loss: 0.06657938\n",
      "29 Train loss: 0.055384055\n",
      "30 Train loss: 0.058819544\n",
      "30 Test accuracy: 0.9664\n",
      "31 Train loss: 0.07524643\n",
      "32 Train loss: 0.05895933\n",
      "33 Train loss: 0.03747204\n",
      "34 Train loss: 0.050341133\n",
      "35 Train loss: 0.03207518\n",
      "35 Test accuracy: 0.9691\n",
      "36 Train loss: 0.053319603\n",
      "37 Train loss: 0.029111093\n",
      "38 Train loss: 0.039535094\n",
      "39 Train loss: 0.0474848\n",
      "40 Train loss: 0.05246562\n",
      "40 Test accuracy: 0.9719\n",
      "41 Train loss: 0.054370437\n",
      "42 Train loss: 0.05284961\n",
      "43 Train loss: 0.04516445\n",
      "44 Train loss: 0.05830694\n",
      "45 Train loss: 0.094185635\n",
      "45 Test accuracy: 0.9725\n",
      "46 Train loss: 0.045582052\n",
      "47 Train loss: 0.083620265\n",
      "48 Train loss: 0.037909918\n",
      "49 Train loss: 0.023676693\n",
      "50 Train loss: 0.039755464\n",
      "50 Test accuracy: 0.9741\n",
      "51 Train loss: 0.045246065\n",
      "52 Train loss: 0.02188055\n",
      "53 Train loss: 0.050396282\n",
      "54 Train loss: 0.048244756\n",
      "55 Train loss: 0.03338233\n",
      "55 Test accuracy: 0.973\n",
      "56 Train loss: 0.03656076\n",
      "57 Train loss: 0.03649858\n",
      "58 Train loss: 0.041134294\n",
      "59 Train loss: 0.044071674\n",
      "60 Train loss: 0.059339322\n",
      "60 Test accuracy: 0.9754\n",
      "61 Train loss: 0.025489334\n",
      "62 Train loss: 0.043309305\n",
      "63 Train loss: 0.039645873\n",
      "64 Train loss: 0.039156396\n",
      "65 Train loss: 0.021862809\n",
      "65 Test accuracy: 0.9739\n",
      "66 Train loss: 0.029873008\n",
      "67 Train loss: 0.026638158\n",
      "68 Train loss: 0.03571645\n",
      "69 Train loss: 0.05076567\n",
      "70 Train loss: 0.029464792\n",
      "70 Test accuracy: 0.9743\n",
      "71 Train loss: 0.023611411\n",
      "72 Train loss: 0.022333229\n",
      "73 Train loss: 0.018414505\n",
      "74 Train loss: 0.03150822\n",
      "75 Train loss: 0.03803495\n",
      "75 Test accuracy: 0.9747\n",
      "76 Train loss: 0.0141191\n",
      "77 Train loss: 0.0141889565\n",
      "78 Train loss: 0.019174099\n",
      "79 Train loss: 0.018237967\n",
      "80 Train loss: 0.02131046\n",
      "80 Test accuracy: 0.9745\n",
      "81 Train loss: 0.03163657\n",
      "82 Train loss: 0.0067349407\n",
      "83 Train loss: 0.015687287\n",
      "84 Train loss: 0.014042481\n",
      "85 Train loss: 0.032452054\n",
      "85 Test accuracy: 0.9733\n",
      "86 Train loss: 0.01372998\n",
      "87 Train loss: 0.016056484\n",
      "88 Train loss: 0.03402633\n",
      "89 Train loss: 0.03798674\n",
      "90 Train loss: 0.016442452\n",
      "90 Test accuracy: 0.9746\n",
      "91 Train loss: 0.013396325\n",
      "92 Train loss: 0.015949814\n",
      "93 Train loss: 0.015761668\n",
      "94 Train loss: 0.013296289\n",
      "95 Train loss: 0.003604313\n",
      "95 Test accuracy: 0.9739\n",
      "96 Train loss: 0.009492738\n",
      "97 Train loss: 0.017798968\n",
      "98 Train loss: 0.021884413\n",
      "99 Train loss: 0.012829087\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 500\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = generate_batch(X_train1, y_train1, batch_size)\n",
    "            loss_val, _ = sess.run([loss, training_op], feed_dict={X: X_batch, y: y_batch})\n",
    "        print(epoch, \"Train loss:\", loss_val)\n",
    "        if epoch % 5 == 0:\n",
    "            acc_test = accuracy.eval(feed_dict={X: X_test1, y: y_test1})\n",
    "            print(epoch, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_digit_comparison_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 10.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing lower layers by using tf.stop.gradient() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 savers, one for DNN A and other for final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "dnn_outputs = dnn(X, name=\"DNN_A\")\n",
    "frozen_outputs = tf.stop_gradient(dnn_outputs)\n",
    "\n",
    "logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init)\n",
    "Y_proba = tf.nn.softmax(logits)\n",
    "\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "dnn_A_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"DNN_A\")\n",
    "restore_saver = tf.train.Saver(var_list={var.op.name: var for var in dnn_A_vars})\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model on small MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_digit_comparison_model.ckpt\n",
      "0 Test accuracy: 0.937\n",
      "10 Test accuracy: 0.9601\n",
      "20 Test accuracy: 0.9638\n",
      "30 Test accuracy: 0.9643\n",
      "40 Test accuracy: 0.9642\n",
      "50 Test accuracy: 0.9642\n",
      "60 Test accuracy: 0.9642\n",
      "70 Test accuracy: 0.9645\n",
      "80 Test accuracy: 0.9647\n",
      "90 Test accuracy: 0.9647\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_digit_comparison_model.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 10 == 0:\n",
    "            acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "            print(epoch, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_mnist_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN classifier without transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "dnn_outputs = dnn(X, name=\"DNN_A\")\n",
    "\n",
    "logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init)\n",
    "Y_proba = tf.nn.softmax(logits)\n",
    "\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "dnn_A_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"DNN_A\")\n",
    "restore_saver = tf.train.Saver(var_list={var.op.name: var for var in dnn_A_vars})\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the accuracy of DNN without transfer learning and with transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.8893\n",
      "10 Test accuracy: 0.9403\n",
      "20 Test accuracy: 0.9476\n",
      "30 Test accuracy: 0.9476\n",
      "40 Test accuracy: 0.948\n",
      "50 Test accuracy: 0.9476\n",
      "60 Test accuracy: 0.9476\n",
      "70 Test accuracy: 0.9472\n",
      "80 Test accuracy: 0.9475\n",
      "90 Test accuracy: 0.9472\n",
      "100 Test accuracy: 0.9472\n",
      "110 Test accuracy: 0.9474\n",
      "120 Test accuracy: 0.9474\n",
      "130 Test accuracy: 0.9474\n",
      "140 Test accuracy: 0.9475\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 150\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 10 == 0:\n",
    "            acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "            print(epoch, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_mnist_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning improved he accuracy value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
